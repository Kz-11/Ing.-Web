<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Acerca de - Reinforcement Learning Hub</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&family=Open+Sans:wght@400;600&display=swap" rel="stylesheet">
</head>
<body>
    <header>
        <div class="container">
            <div class="logo-container">
                <div class="logo">
                    <span class="logo-icon">🤖</span>
                    <span class="logo-text">RL Hub</span>
                </div>
            </div>
            <nav>
                <ul>
                    <li><a href="index.html">Inicio</a></li>
                    <li><a href="about.html" class="active">Acerca de</a></li>
                    <li><a href="contact.html">Contacto</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main class="container">
        <section class="hero">
            <h1>Acerca de Reinforcement Learning</h1>
            <p>Descubre la historia, fundamentos y aplicaciones del aprendizaje por refuerzo</p>
        </section>

        <div class="content-wrapper">
            <div class="main-content">
                <article class="card">
                    <h2>Historia del Reinforcement Learning</h2>
                    <p>El <strong>Aprendizaje por Refuerzo</strong> tiene sus raíces en múltiples disciplinas, incluyendo la psicología conductual, la teoría de control y la teoría de juegos. Su evolución ha sido marcada por hitos significativos:</p>
                    
                    <div class="timeline">
                        <div class="timeline-item">
                            <div class="timeline-date">1950s</div>
                            <div class="timeline-content">
                                <h4>Orígenes en la Psicología</h4>
                                <p>Los trabajos de B.F. Skinner sobre condicionamiento operante sentaron las bases conceptuales del aprendizaje por refuerzo.</p>
                            </div>
                        </div>
                        
                        <div class="timeline-item">
                            <div class="timeline-date">1980s</div>
                            <div class="timeline-content">
                                <h4>Desarrollo Teórico</h4>
                                <p>Richard Sutton y Andrew Barto publican trabajos fundamentales que establecen las bases matemáticas del RL moderno.</p>
                            </div>
                        </div>
                        
                        <div class="timeline-item">
                            <div class="timeline-date">1992</div>
                            <div class="timeline-content">
                                <h4>Q-Learning</h4>
                                <p>Chris Watkins desarrolla el algoritmo Q-Learning, un hito fundamental en los métodos de aprendizaje sin modelo.</p>
                            </div>
                        </div>
                        
                        <div class="timeline-item">
                            <div class="timeline-date">2013</div>
                            <div class="timeline-content">
                                <h4>Deep Q-Network</h4>
                                <p>DeepMind presenta DQN, combinando redes neuronales profundas con Q-Learning, logrando superar a humanos en juegos de Atari.</p>
                            </div>
                        </div>
                        
                        <div class="timeline-item">
                            <div class="timeline-date">2016</div>
                            <div class="timeline-content">
                                <h4>AlphaGo</h4>
                                <p>DeepMind hace historia cuando AlphaGo vence al campeón mundial Lee Sedol en el juego de Go.</p>
                            </div>
                        </div>
                        
                        <div class="timeline-item">
                            <div class="timeline-date">Presente</div>
                            <div class="timeline-content">
                                <h4>Era de Aplicaciones Prácticas</h4>
                                <p>El RL se aplica en robótica, vehículos autónomos, sistemas de recomendación, finanzas y más.</p>
                            </div>
                        </div>
                    </div>
                </article>

                <article class="card">
                    <h2>Fundamentos Teóricos</h2>
                    <p>El aprendizaje por refuerzo se basa en el <strong>proceso de decisión de Markov (MDP)</strong>, que proporciona el marco matemático para modelar problemas de toma de decisiones secuenciales.</p>
                    
                    <h3>Elementos Clave de un MDP</h3>
                    <div class="mdp-elements">
                        <div class="mdp-element">
                            <h4>Estado (S)</h4>
                            <p>Representación del entorno en un momento dado</p>
                        </div>
                        <div class="mdp-element">
                            <h4>Acciones (A)</h4>
                            <p>Conjunto de decisiones disponibles para el agente</p>
                        </div>
                        <div class="mdp-element">
                            <h4>Función de Transición (P)</h4>
                            <p>Probabilidad de transición entre estados dada una acción</p>
                        </div>
                        <div class="mdp-element">
                            <h4>Función de Recompensa (R)</h4>
                            <p>Señal numérica que indica la calidad de una acción</p>
                        </div>
                        <div class="mdp-element">
                            <h4>Factor de Descuento (γ)</h4>
                            <p>Balance entre recompensas inmediatas y futuras</p>
                        </div>
                    </div>
                    
                    <h3>Ecuación de Bellman</h3>
                    <p>La ecuación fundamental que define la relación recursiva entre los valores de estados consecutivos:</p>
                    <div class="equation">
                        V(s) = max<sub>a</sub> [R(s,a) + γ Σ<sub>s'</sub> P(s'|s,a) V(s')]
                    </div>
                    <p class="equation-caption">Donde V(s) es el valor del estado s, R(s,a) es la recompensa, γ es el factor de descuento, y P(s'|s,a) es la probabilidad de transición.</p>
                </article>

                <article class="card">
                    <h2>Aplicaciones en la Industria</h2>
                    <p>El RL ha trascendido el ámbito académico para convertirse en una tecnología transformadora en múltiples industrias:</p>
                    
                    <div class="applications-detailed">
                        <div class="application-detailed">
                            <h3>🏭 Manufactura y Logística</h3>
                            <p><strong>Optimización de procesos:</strong> Los algoritmos de RL optimizan cadenas de producción, reduciendo tiempos de inactividad y mejorando la eficiencia energética.</p>
                            <p><strong>Gestión de inventarios:</strong> Sistemas que aprenden patrones de demanda para optimizar niveles de stock y reducir costos de almacenamiento.</p>
                        </div>
                        
                        <div class="application-detailed">
                            <h3>💻 Tecnología y TI</h3>
                            <p><strong>Asistentes virtuales:</strong> Mejora de conversaciones naturales mediante el aprendizaje de respuestas más efectivas.</p>
                            <p><strong>Gestión de recursos en la nube:</strong> Asignación dinámica de recursos computacionales basada en patrones de uso.</p>
                            <p><strong>Ciberseguridad:</strong> Detección de amenazas mediante el aprendizaje de patrones de ataque.</p>
                        </div>
                        
                        <div class="application-detailed">
                            <h3>🎮 Entretenimiento y Juegos</h3>
                            <p><strong>NPCs inteligentes:</strong> Personajes no jugadores que aprenden y adaptan su comportamiento.</p>
                            <p><strong>Generación de contenido:</strong> Creación procedural de niveles y misiones basada en preferencias del jugador.</p>
                            <p><strong>Testing automatizado:</strong> Agentes que prueban juegos buscando bugs y desbalances.</p>
                        </div>
                        
                        <div class="application-detailed">
                            <h3>🏥 Salud y Medicina</h3>
                            <p><strong>Tratamientos personalizados:</strong> Dosificación adaptativa de medicamentos basada en respuesta del paciente.</p>
                            <p><strong>Diagnóstico asistido:</strong> Sistemas que aprenden a identificar patrones en imágenes médicas.</p>
                            <p><strong>Robótica quirúrgica:</strong> Movimientos precisos aprendidos mediante simulación.</p>
                        </div>
                    </div>
                </article>

                <article class="card">
                    <h2>Investigación Actual y Futuro</h2>
                    <p>El campo del RL continúa evolucionando rápidamente, con varias áreas de investigación prometedoras:</p>
                    
                    <div class="research-areas">
                        <div class="research-area">
                            <h4>🤝 RL Multiagente</h4>
                            <p>Desarrollo de sistemas donde múltiples agentes aprenden a colaborar o competir en entornos complejos.</p>
                        </div>
                        
                        <div class="research-area">
                            <h4>🧠 RL con Modelos del Mundo</h4>
                            <p>Agentes que aprenden modelos internos del entorno para planificar y tomar decisiones más informadas.</p>
                        </div>
                        
                        <div class="research-area">
                            <h4>⚡ RL de Muestreo Eficiente</h4>
                            <p>Métodos que reducen la cantidad de interacciones necesarias con el entorno para aprender políticas efectivas.</p>
                        </div>
                        
                        <div class="research-area">
                            <h4>🔒 RL Seguro y Robusto</h4>
                            <p>Desarrollo de algoritmos que garantizan comportamientos seguros incluso en condiciones no vistas durante el entrenamiento.</p>
                        </div>
                        
                        <div class="research-area">
                            <h4>🌐 RL en Entornos del Mundo Real</h4>
                            <p>Adaptación de algoritmos para funcionar en entornos ruidosos, parcialmente observables y de alta dimensión.</p>
                        </div>
                    </div>
                </article>
            </div>

            <aside class="sidebar">
                <div class="card">
                    <h3>Pioneros del RL</h3>
                    <div class="pioneer">
                        <h4>Richard Sutton</h4>
                        <p>Considerado el padre del RL moderno, autor del libro fundamental "Reinforcement Learning: An Introduction".</p>
                    </div>
                    <div class="pioneer">
                        <h4>Andrew Barto</h4>
                        <p>Coautor con Sutton y contribuidor fundamental a las bases teóricas del campo.</p>
                    </div>
                    <div class="pioneer">
                        <h4>David Silver</h4>
                        <p>Líder de investigación en DeepMind y arquitecto principal de AlphaGo y AlphaZero.</p>
                    </div>
                    <div class="pioneer">
                        <h4>Pieter Abbeel</h4>
                        <p>Pionero en la aplicación de RL a la robótica y fundador de Covariant.</p>
                    </div>
                </div>

                <div class="card">
                    <h3>Instituciones Líderes</h3>
                    <ul class="institutions-list">
                        <li>DeepMind (Google)</li>
                        <li>OpenAI</li>
                        <li>Berkeley AI Research</li>
                        <li>MIT CSAIL</li>
                        <li>Stanford AI Lab</li>
                        <li>Carnegie Mellon University</li>
                    </ul>
                </div>

                <div class="card">
                    <h3>Logros Destacados</h3>
                    <div class="achievement">
                        <h4>AlphaGo (2016)</h4>
                        <p>Primer programa en vencer a un campeón humano profesional en Go.</p>
                    </div>
                    <div class="achievement">
                        <h4>OpenAI Five (2019)</h4>
                        <p>Equipo de IA que venció al campeón mundial de Dota 2.</p>
                    </div>
                    <div class="achievement">
                        <h4>AlphaFold (2020)</h4>
                        <p>Revolución en el plegamiento de proteínas usando técnicas de RL.</p>
                    </div>
                    <div class="achievement">
                        <h4>MuZero (2020)</h4>
                        <p>Algoritmo que domina múltiples juegos sin conocer las reglas.</p>
                    </div>
                </div>
            </aside>
        </div>
    </main>

    <footer>
        <div class="container">
            <div class="footer-content">
                <div class="footer-section">
                    <div class="logo">
                        <span class="logo-icon">🤖</span>
                        <span class="logo-text">RL Hub</span>
                    </div>
                    <p>Explorando el futuro del aprendizaje automático a través del reinforcement learning.</p>
                </div>
                
                <div class="footer-section">
                    <h4>Enlaces Rápidos</h4>
                    <ul>
                        <li><a href="index.html">Inicio</a></li>
                        <li><a href="about.html">Acerca de</a></li>
                        <li><a href="contact.html">Contacto</a></li>
                    </ul>
                </div>
                
                <div class="footer-section">
                    <h4>Síguenos</h4>
                    <div class="social-links">
                        <a href="https://twitter.com/" target="_blank" aria-label="Twitter">
                            <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                                <path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723 10.016 10.016 0 01-3.127 1.195 4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.937 4.937 0 004.604 3.417 9.868 9.868 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63a9.936 9.936 0 002.46-2.543l-.047-.02z"/>
                            </svg>
                        </a>
                        <a href="https://www.linkedin.com/" target="_blank" aria-label="LinkedIn">
                            <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                                <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                            </svg>
                        </a>
                        <a href="https://www.instagram.com/" target="_blank" aria-label="Instagram">
                            <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                                <path d="M12 2.163c3.204 0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849 0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204 0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849 0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zm0-2.163c-3.259 0-3.667.014-4.947.072-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948 0 3.259.014 3.668.072 4.948.2 4.358 2.618 6.78 6.98 6.98 1.281.058 1.689.072 4.948.072 3.259 0 3.668-.014 4.948-.072 4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948 0-3.259-.014-3.667-.072-4.947-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403 0-6.162 2.759-6.162 6.162s2.759 6.163 6.162 6.163 6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zm0 10.162c-2.209 0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796 0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795 0 1.439-.645 1.439-1.44s-.644-1.44-1.439-1.44z"/>
                            </svg>
                        </a>
                    </div>
                </div>
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2025 Reinforcement Learning Hub. Todos los derechos reservados.</p>
            </div>
        </div>
    </footer>
</body>

</html>
